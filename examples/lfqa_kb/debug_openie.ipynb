{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Coreference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f5914461f90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS \\\n",
    "                                                or token.lemma_ in STOP_WORDS\n",
    "Token.set_extension('is_stop', getter=stop_words_getter, force=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "ruler = EntityRuler(nlp)\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. load Trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marisa_trie\n",
    "# with open (\"/dccstor/myu/experiments/knowledge_trie/openie_dev/full_str_20_5.txt\") as f:\n",
    "#     # full_str_list = [line.strip().split('\\t') for line in f]\n",
    "#     full_str_list = [\" \".join(line.strip().split('\\t')[:2]) for line in f]\n",
    "#     ext_trie = marisa_trie.Trie(full_str_list)\n",
    "\n",
    "\n",
    "import pickle\n",
    "ext_trie = pickle.load(open('/dccstor/myu/experiments/knowledge_trie/openie_dev/dummy_trie.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"trie\": ext_trie}\n",
    "pickle.dump(a, open('/dccstor/myu/experiments/knowledge_trie/openie_dev/dummy_id2trie.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['driving while under influence of impairing substance driving',\n",
       " 'driving while under influence of impairing substance driving Drug',\n",
       " 'driving while under influence of impairing substance impaired driving',\n",
       " 'driving while under influence of impairing substance impaired driving Drug',\n",
       " 'driving while under influence of substance driving',\n",
       " 'driving while under influence of substance driving Drug',\n",
       " 'driving while under influence of substance impaired driving',\n",
       " 'driving while under influence of substance impaired driving Drug',\n",
       " 'driving while under influence driving',\n",
       " 'driving while under influence driving Drug',\n",
       " 'driving while under influence impaired driving',\n",
       " 'driving while under influence impaired driving Drug',\n",
       " 'driving while driving',\n",
       " 'driving while driving Drug',\n",
       " 'driving while impaired driving',\n",
       " 'driving while impaired driving Drug',\n",
       " 'driving Cannabinoids effects',\n",
       " 'driving Cannabinoids effects on driver',\n",
       " 'driving Cannabinoids effects on driver similar',\n",
       " 'driving Cannabinoids effects on driver similar to those',\n",
       " 'driving Cannabinoids effects on driver similar to those of alcohol',\n",
       " 'driving Cannabinoids driving',\n",
       " 'driving Cannabinoids impaired driving',\n",
       " 'driving Cannabinoids users',\n",
       " 'driving driving',\n",
       " 'driving driving while',\n",
       " 'driving driving while under influence',\n",
       " 'driving driving while under influence of impairing substance',\n",
       " 'driving driving while under influence of substance',\n",
       " 'driving driving Cannabinoids',\n",
       " 'driving driving Drug',\n",
       " 'driving Drug driving',\n",
       " 'driving Drug driving while',\n",
       " 'driving Drug driving while under influence',\n",
       " 'driving Drug driving while under influence of impairing substance',\n",
       " 'driving Drug driving while under influence of substance',\n",
       " 'driving impaired driving',\n",
       " 'driving impaired driving Cannabinoids',\n",
       " 'driving impaired driving Drug',\n",
       " 'driving Utah Code Section']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_trie.keys(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dccstor/myu/experiments/knowledge_trie/asqa_dev/graph_3_0-10.gv.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import graphviz\n",
    "g = graphviz.Digraph('G', filename=\"/dccstor/myu/experiments/knowledge_trie/asqa_dev/graph_3_0-100.gv\")\n",
    "edge_set = set()\n",
    "for line in full_str_list:\n",
    "    tokens = line.split(' ')\n",
    "    for i in range(len(tokens)-1):\n",
    "        if f\"{tokens[i]} {tokens[i+1]}\" not in edge_set:\n",
    "            g.edge(tokens[i], tokens[i+1])\n",
    "            edge_set.add(f\"{tokens[i]} {tokens[i+1]}\")\n",
    "g.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dccstor/myu/experiments/knowledge_trie/asqa_dev/graph_3_0-100_subjobj.gv.pdf'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import graphviz\n",
    "g = graphviz.Digraph('G', filename=\"/dccstor/myu/experiments/knowledge_trie/asqa_dev/graph_3_0-100_subjobj.gv\")\n",
    "edge_set = set()\n",
    "for subj,obj,score in full_str_list:\n",
    "    tokens = line.split(' ')\n",
    "  \n",
    "    if f\"{subj} {obj}\" not in edge_set:\n",
    "        g.edge(subj, obj)\n",
    "        edge_set.add(f\"{subj} {obj}\" )\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does VERB\n",
      "marijuana NOUN\n",
      "impair VERB\n",
      "driving NOUN\n",
      "ability NOUN\n",
      "? PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(\"Does marijuana impair driving ability?\"):\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. query from Trie\n",
    "In the KID paper they use max_hops=3 and the window size for local memory is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial local memory: ['ability', 'driving', 'marijuana']\n",
      ">>> new knowledge from hop 1 : 23\n",
      "{'while', 'impaired', 'Cannabinoids', 'alcohol', 'Code', 'driving', 'impairing', 'driver', 'influence', 'effects', 'marijuana', 'to', 'users', 'substance', 'Utah', 'role', 'those', 'Section', 'similar', 'under', 'of', 'Drug', 'on'}\n",
      ">>> new knowledge from hop 2 : 92\n",
      "{'law', '-9-tetahydrocannabinol', 'impaired', 'by', 'test', 'least', 'person', 'crashes', 'after', 'often', 'conducted', 'marijuana', 'role', 'for', 'those', 'blood', 'fluids', 'intoxication', '10', '58', 'ml', 'on', 'played', 'no', 'THC', 'Cannabinoids', 'convicted', 'obtain', 'driving', 'impairing', 'driver', 'with', 'drivers', 'more', 'enforcement', 'Delta', 'to', 'its', 'site', 'detectable', 'Utah', 'their', 'definition', 'substantial', 'of', 'Drug', 'Ohio', 'National', 'accurate', 'because', 'statute', 'likely', 'prescription', 'alcohol', 'Code', 'detected', 'legal', 'body', 'amount', 'impairment', 'ng', 'illicit', 'it', 'cannabis', 'users', 'Section', 'similar', \"'s\", 'under', 'car', 'while', 'drug', 'context', 'specific', 'concentration', 'fluid', 'influence', 'urine', 'at', 'effects', 'oral', 'Institute', 'kits', 'substance', 'drive', 'Abuse', 'in', 'Statute', 'unclear', 'days', 'collect', 'drugs'}\n"
     ]
    }
   ],
   "source": [
    "import marisa_trie\n",
    "with open (\"/dccstor/myu/experiments/knowledge_trie/openie_dev/full_str_20_5.txt\") as f:\n",
    "    # full_str_list = [line.strip().split('\\t') for line in f]\n",
    "    full_str_list = [\" \".join(line.strip().split('\\t')[:2]) for line in f]\n",
    "    ext_trie = marisa_trie.Trie(full_str_list)\n",
    "\n",
    "max_hops = 2\n",
    "question = \"Does marijuana impair driving ability?\"\n",
    "cur_gen_toks = question\n",
    "local_kg = [\n",
    "        token.lemma_.lower() for token in nlp(cur_gen_toks)\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop\n",
    "]\n",
    "local_kg = list(set(local_kg))\n",
    "print(\"initial local memory:\", local_kg)\n",
    "tmp_kg = local_kg\n",
    "related_kgs = set() # all accquired words through multi-hop querying, will serve as the vocab to guide next step generation\n",
    "for i in range(max_hops):\n",
    "    new_knowledge = [] # record new words that are queried from this hop\n",
    "    for ent in tmp_kg:\n",
    "        for span in ext_trie.keys(ent):\n",
    "            new_knowledge.extend(span.split(' '))\n",
    "    new_knowledge = set(new_knowledge)\n",
    "    print(\">>> new knowledge from hop\", i+1,\":\",len(new_knowledge))\n",
    "    print(new_knowledge)\n",
    "    tmp_kg = list(new_knowledge) # reset tmp queries as new knowledge for next hop\n",
    "    related_kgs |= new_knowledge\n",
    "\n",
    "# print(\"queried knowledge demonstrations\", list(related_kgs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marisa_trie\n",
    "with open (\"/dccstor/myu/experiments/knowledge_trie/asqa_dev/full_str_3_0-10.txt\") as f:\n",
    "    # full_str_list = [line.strip().split('\\t') for line in f]\n",
    "    full_str_list = [\" \".join(line.strip().split('\\t')[:2]) for line in f]\n",
    "    ext_trie = marisa_trie.Trie(full_str_list)\n",
    "\n",
    "max_hops = 2\n",
    "question = \"Who has the highest goals in world football?\"\n",
    "cur_gen_toks = question\n",
    "local_kg = [\n",
    "        token.lemma_.lower() for token in nlp(cur_gen_toks)\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop\n",
    "]\n",
    "local_kg = list(set(local_kg))\n",
    "print(\"initial local memory:\", local_kg)\n",
    "tmp_kg = local_kg\n",
    "related_kgs = set() # all accquired words through multi-hop querying, will serve as the vocab to guide next step generation\n",
    "for i in range(max_hops):\n",
    "    new_knowledge = [] # record new words that are queried from this hop\n",
    "    for ent in tmp_kg:\n",
    "        for span in ext_trie.keys(ent):\n",
    "            new_knowledge.extend(span.split(' '))\n",
    "    new_knowledge = set(new_knowledge)\n",
    "    print(\">>> new knowledge from hop\", i+1,\":\",len(new_knowledge))\n",
    "    print(new_knowledge)\n",
    "    tmp_kg = list(new_knowledge) # reset tmp queries as new knowledge for next hop\n",
    "    related_kgs |= new_knowledge\n",
    "\n",
    "# print(\"queried knowledge demonstrations\", list(related_kgs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marisa_trie\n",
    "import pickle\n",
    "trie = pickle.load(open(\"/dccstor/myu/experiments/knowledge_trie/eli5_openie_dev_pkl/id2kg_0_1507.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial local memory: ['poisoning', 'beef', 'flesh', 'food', 'chicken', 'poisoning?why', 'salmon', 'animal', 'pork']\n",
      ">>> new knowledge from hop 1 : 20\n",
      "{'food', 'salmonella', 'commercial', 'raw', 'range', 'wide', 'bacteria', 'poisoning', 'malnutrition', 'chicken', 'foodborne', 'contaminated', 'animal', 'illness', 'value', 'adulterated', 'considered', 'shrimp', 'articles', 'found'}\n",
      ">>> new knowledge from hop 2 : 10\n",
      "{'seen', 'spp', 'products', 'bacterial', 'cl', 'contamination', 'infecting', '.', 'genera', 'meat'}\n"
     ]
    }
   ],
   "source": [
    "ext_trie = trie[\"1ircew\"]\n",
    "max_hops = 2\n",
    "question = \"Why are some animals' flesh (beef, salmon, etc.) fine to eat raw, whilst others (chicken, pork, etc.) cause food poisoning?\"\"Why are some animals' flesh (beef, salmon, etc.) fine to eat raw, whilst others (chicken, pork, etc.) cause food poisoning?\"\n",
    "cur_gen_toks = question\n",
    "local_kg = [\n",
    "        token.lemma_.lower() for token in nlp(cur_gen_toks)\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop\n",
    "]\n",
    "local_kg = list(set(local_kg))\n",
    "print(\"initial local memory:\", local_kg)\n",
    "tmp_kg = local_kg\n",
    "related_kgs = set() # all accquired words through multi-hop querying, will serve as the vocab to guide next step generation\n",
    "for i in range(max_hops):\n",
    "    new_knowledge = [] # record new words that are queried from this hop\n",
    "    for ent in tmp_kg:\n",
    "        for span in ext_trie.keys(ent):\n",
    "            new_knowledge.extend(span.split(' '))\n",
    "    new_knowledge = set(new_knowledge)\n",
    "    new_knowledge -= related_kgs\n",
    "    print(\">>> new knowledge from hop\", i+1,\":\",len(new_knowledge))\n",
    "    print(new_knowledge)\n",
    "    tmp_kg = list(new_knowledge) # reset tmp queries as new knowledge for next hop\n",
    "    related_kgs |= new_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial local memory: ['mirror', 'night', 'view', 'work']\n",
      ">>> new knowledge from hop 1 : 55\n",
      "{'mirror', 'affixed', 'catetus', 'unequal', 'pass', 'mirrors', 'mount', 'helmet', 'automobiles', 'user', 'positioned', 'image', 'frame', 'line', 'area', 'usually', 'windshield', 'tilt', 'appears', 'recently', 'equipped', 'double', 'video', '1970s', 'early', 'position', '1930s', 'bicycle', 'new', 'driver', 'built', 'night', 'swivel', 'allowing', 'material', 'large', 'screen', 'model', 'spectator', 'moved', 'vehicle', 'cameras', 'fitted', 'view', 'resulting', 'additional', 'convex', 'designed', 'tilted', 'purchase', 'unviewable', 'day', 'manual', 'cars', 'arm'}\n",
      ">>> new knowledge from hop 2 : 22\n",
      "{'turn', 'bicycles', 'surface', 'seen', 'actually', 'point', 'tilting', 'eyes', 'license', 'fixed', 'perfect', 'window', 'reflection', 'signal', 'handlebar', 'low', 'rear', 'repeaters', 'technique', 'mounted', 'cinematographic', 'stays'}\n"
     ]
    }
   ],
   "source": [
    "ext_trie = trie[\"3ji34o\"]\n",
    "max_hops = 2\n",
    "question = \"How does the rear view mirror work after flipping it up at night?\"\n",
    "cur_gen_toks = question\n",
    "local_kg = [\n",
    "        token.lemma_.lower() for token in nlp(cur_gen_toks)\n",
    "        if token.pos_ in ['PROPN', 'NOUN'] and not token.is_stop\n",
    "]\n",
    "local_kg = list(set(local_kg))\n",
    "print(\"initial local memory:\", local_kg)\n",
    "tmp_kg = local_kg\n",
    "related_kgs = set() # all accquired words through multi-hop querying, will serve as the vocab to guide next step generation\n",
    "for i in range(max_hops):\n",
    "    new_knowledge = [] # record new words that are queried from this hop\n",
    "    for ent in tmp_kg:\n",
    "        for span in ext_trie.keys(ent):\n",
    "            new_knowledge.extend(span.split(' '))\n",
    "    new_knowledge = set(new_knowledge)\n",
    "    new_knowledge -= related_kgs\n",
    "    print(\">>> new knowledge from hop\", i+1,\":\",len(new_knowledge))\n",
    "    print(new_knowledge)\n",
    "    tmp_kg = list(new_knowledge) # reset tmp queries as new knowledge for next hop\n",
    "    related_kgs |= new_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "lm_logits = torch.rand(4,2,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4987, 0.1781, 0.9329, 0.1802, 0.4110],\n",
       "         [0.5884, 0.0087, 0.6253, 0.0630, 0.5300]],\n",
       "\n",
       "        [[0.4661, 0.9510, 0.3345, 0.2594, 0.4666],\n",
       "         [0.5377, 0.5169, 0.0254, 0.7068, 0.5137]],\n",
       "\n",
       "        [[0.3831, 0.2606, 0.4062, 0.0303, 0.5544],\n",
       "         [0.6495, 0.6991, 0.7450, 0.9325, 0.3657]],\n",
       "\n",
       "        [[0.3691, 0.1282, 0.6169, 0.6170, 0.2063],\n",
       "         [0.4647, 0.5559, 0.3278, 0.1534, 0.2684]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0256, 0.5137, 0.1751, 0.1179, 0.2028],\n",
       "        [0.9846, 0.8263, 0.7400, 0.6195, 0.8569],\n",
       "        [0.4272, 0.9955, 0.4984, 0.2734, 0.7437],\n",
       "        [0.3437, 0.1794, 0.9025, 0.6721, 0.1655]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_embeds = torch.rand(4,5)\n",
    "kg_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_embeds = [x for x in kg_embeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0256, 0.5137, 0.1751, 0.1179, 0.2028]),\n",
       " tensor([0.9846, 0.8263, 0.7400, 0.6195, 0.8569]),\n",
       " tensor([0.4272, 0.9955, 0.4984, 0.2734, 0.7437]),\n",
       " tensor([0.3437, 0.1794, 0.9025, 0.6721, 0.1655])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0256, 0.5137, 0.1751, 0.1179, 0.2028],\n",
       "        [0.9846, 0.8263, 0.7400, 0.6195, 0.8569],\n",
       "        [0.4272, 0.9955, 0.4984, 0.2734, 0.7437],\n",
       "        [0.3437, 0.1794, 0.9025, 0.6721, 0.1655]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(kg_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = lm_logits/torch.sum(lm_logits, dim=-1).unsqueeze(-1).expand_as(lm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000],\n",
       "        [1.0000, 1.0000],\n",
       "        [1.0000, 1.0000],\n",
       "        [1.0000, 1.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(new, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9881, 0.9881, 0.9881, 0.9881]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = [0,1,2,3]\n",
    "max_logits = torch.max(lm_logits[0], dim=-1)[0].unsqueeze(-1).expand(lm_logits.shape[1], len(token_ids))\n",
    "max_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kg_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1194373/4015622819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkg_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kg_logits' is not defined"
     ]
    }
   ],
   "source": [
    "kg_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('lfqa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88ef27eae4435836562f365b00201516abaa7c67ef051fce85d1491a271aa183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
