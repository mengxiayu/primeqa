output_dir=/dccstor/myu/experiments/asqa_dprbart_beam_5e05
jbsub -q x86_24h -cores 4+1 -mem 32g -require 'v100' -proj 'asqa' -name 'train_asqa_dprbart' -o ${output_dir}/train.log \
python /dccstor/myu/primeqa/examples/lfqa_kb/run_lfqa.py \
  --model_name_or_path facebook/bart-large \
  --dataset_name asqa \
  --train_file /dccstor/myu/data/asqa_dpr/asqa_train_dpr.json \
  --validation_file /dccstor/myu/data/asqa_dpr/asqa_dev_dpr.json \
  --question_column ambiguous_question \
  --answer_column annotations \
  --do_train \
  --do_eval \
  --per_device_train_batch_size 8 \
  --per_device_eval_batch_size 8 \
  --gradient_accumulation_steps 4 \
  --learning_rate 5e-5 \
  --num_train_epochs 10 \
  --max_seq_length 512 \
  --max_answer_length 128 \
  --generation_max_length 128 \
  --fp16 \
  --predict_with_generate \
  --output_dir ${output_dir} \
  --overwrite_output_dir \
  --overwrite_cache \
  --evaluation_strategy epoch \
  --save_strategy epoch \
  --load_best_model_at_end \
  --metric_for_best_model rougeL \
  --save_total_limit 3 \
  --context_column passages \
  --n_context 5 \
  --generation_num_beams 4 \
  --preprocessing_num_workers 4 \
